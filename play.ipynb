{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10cecca-6791-4779-bc96-48b929ce01ef",
   "metadata": {},
   "source": [
    "# Ver jugar a un modelo de Super Mario preentrenado\n",
    "\n",
    "https://github.com/jorgecasase/mariobrosplay-windows-gym-RL\n",
    "\n",
    "Este notebook permite cargar y observar cómo juega un modelo de Super Mario preentrenado. Utiliza un entorno personalizado basado en OpenAI Gym, junto con varias optimizaciones específicas para juegos retro.\n",
    "\n",
    "## Requisitos\n",
    "1. Haber creado el entorno virutal con conda\n",
    "\n",
    "2. Un modelo preentrenado debe estar disponible en el archivo `trained_mario.chkpt`.\n",
    "   - Si no tienes un modelo preentrenado, puedes entrenarlo siguiendo el tutorial en el siguiente enlace:\n",
    "     [Mario RL con PyTorch - Entrenamiento en 400 episodios](https://github.com/pedroconcejero/deep_learning_2024/blob/main/mario_RL_pytorch_tutorial_400_episodes_save_every_1e4.ipynb)\n",
    "     - o puedes usar el de github\n",
    "2. Asegúrate de tener instaladas todas las dependencias necesarias, incluidas `gym`, `gym_super_mario_bros`, y `torch`.\n",
    "\n",
    "Una vez cumplidos los requisitos, puedes cargar el modelo y observar su rendimiento en el entorno.\n",
    "\n",
    "Nota: este notebook está adaptado para ser ejecutado en windows nativo, no funciona en colab ni en macos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303ed85-4414-40da-88aa-5314b96e0178",
   "metadata": {},
   "source": [
    "## clases de juego Neural, MetricLogger, Mario, wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7110b901-cecc-4f43-8e98-eddf578046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "class MarioNet(nn.Module):\n",
    "    '''mini cnn structure\n",
    "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == 'online':\n",
    "            return self.online(input)\n",
    "        elif model == 'target':\n",
    "            return self.target(input)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "import gym\n",
    "import datetime\n",
    "from skimage import transform\n",
    "from collections import deque\n",
    "from gym.spaces import Box\n",
    "\n",
    "class MetricLogger():\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()\n",
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        self.curr_step = 0\n",
    "        self.burnin = 1e5  # min. experiences before training\n",
    "        self.learn_every = 3   # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4   # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "        self.save_every = 5e5   # no. of experiences between saving Mario Net\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device='cuda')\n",
    "        if checkpoint:\n",
    "            self.load(checkpoint)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "        Inputs:\n",
    "        state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "        Outputs:\n",
    "        action_idx (int): An integer representing which action Mario will perform\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model='online')\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
    "        reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
    "        done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
    "\n",
    "        self.memory.append( (state, next_state, action, reward, done,) )\n",
    "\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model='online')[np.arange(0, self.batch_size), action] # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model='online')\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target) :\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        save_path = self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=self.net.state_dict(),\n",
    "                exploration_rate=self.exploration_rate\n",
    "            ),\n",
    "            save_path\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "\n",
    "    def load(self, load_path):\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "\n",
    "        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n",
    "        exploration_rate = ckp.get('exploration_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        self.exploration_rate = exploration_rate\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        resize_obs = transform.resize(observation, self.shape)\n",
    "        # cast float back to uint8\n",
    "        resize_obs *= 255\n",
    "        resize_obs = resize_obs.astype(np.uint8)\n",
    "        return resize_obs\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b009a-1cc6-423b-a991-85eedf6bca79",
   "metadata": {},
   "source": [
    "## Importación de modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3551dfb9-25bb-4523-9f2c-65f370725985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de módulos necesarios\n",
    "import random, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Importación de librerías para Gym y Super Mario\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from nes_py.wrappers import JoypadSpace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c749d5e-e651-447c-869b-5bd556789812",
   "metadata": {},
   "source": [
    "## Creación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2362891-1ace-4374-a7f9-21ddcb709f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del entorno base de Super Mario\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "\n",
    "# Configuración del espacio de acciones (Joypad)\n",
    "env = JoypadSpace(\n",
    "    env,\n",
    "    [['right'],  # Acción 1: caminar a la derecha\n",
    "    ['right', 'A']]  # Acción 2: caminar a la derecha y saltar\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f5aaa-819d-49aa-995f-bb858abcf433",
   "metadata": {},
   "source": [
    "## Aplicación de wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418ee118-42a5-4b70-b0bc-0a9eeb56cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.wrappers.frame_stack.LazyFrames at 0x195fe67bac0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicación de wrappers para modificar el entorno\n",
    "env = SkipFrame(env, skip=4)  # Saltar frames para mejorar el rendimiento\n",
    "env = GrayScaleObservation(env, keep_dim=False)  # Convertir observaciones a escala de grises\n",
    "env = ResizeObservation(env, shape=84)  # Redimensionar observaciones a 84x84 píxeles\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)  # Normalizar observaciones\n",
    "env = FrameStack(env, num_stack=4)  # Apilar 4 frames consecutivos\n",
    "\n",
    "# Reiniciar el entorno para comenzar\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d60df-2569-405b-a9de-79217071f9a7",
   "metadata": {},
   "source": [
    "## Cargar modelo preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6344dfa9-9815-425a-b692-04ca2daf33ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge Casas\\AppData\\Local\\Temp\\ipykernel_7392\\232289090.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model at trained_mario.chkpt with exploration rate 0.1\n"
     ]
    }
   ],
   "source": [
    "# Configuración del directorio de guardado para puntos de control y métricas\n",
    "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)  # Crear el directorio si no existe\n",
    "\n",
    "# Inicialización del agente Mario con un punto de control preentrenado **AQUI CARGAS TU MODELO**\n",
    "checkpoint = Path('trained_mario.chkpt') #<-- AQUI IMPORTA TU MODELO\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint)\n",
    "\n",
    "# Fijar la tasa de exploración al mínimo\n",
    "mario.exploration_rate = mario.exploration_rate_min\n",
    "\n",
    "# Inicialización del registro de métricas\n",
    "logger = MetricLogger(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63268b9-4e6b-49e7-8091-6d115c88387d",
   "metadata": {},
   "source": [
    "## Verlo jugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbaed87-f499-44dc-94e0-de35e3641ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge Casas\\AppData\\Local\\Temp\\ipykernel_7392\\232289090.py:204: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
      "C:\\miniconda3\\envs\\mario\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
      "C:\\Users\\Jorge Casas\\AppData\\Local\\Temp\\ipykernel_7392\\232289090.py:232: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 156 - Epsilon 0.1 - Mean Reward 636.0 - Mean Length 156.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 4.069 - Time 2024-12-12T02:19:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\miniconda3\\envs\\mario\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
      "C:\\Users\\Jorge Casas\\AppData\\Local\\Temp\\ipykernel_7392\\232289090.py:232: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 - Step 5167 - Epsilon 0.1 - Mean Reward 1184.048 - Mean Length 246.048 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 101.049 - Time 2024-12-12T02:20:53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\miniconda3\\envs\\mario\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
      "C:\\Users\\Jorge Casas\\AppData\\Local\\Temp\\ipykernel_7392\\232289090.py:232: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40 - Step 9586 - Epsilon 0.1 - Mean Reward 1177.146 - Mean Length 233.805 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 89.345 - Time 2024-12-12T02:22:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\miniconda3\\envs\\mario\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
      "C:\\Users\\Jorge Casas\\AppData\\Local\\Temp\\ipykernel_7392\\232289090.py:232: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60 - Step 13338 - Epsilon 0.1 - Mean Reward 1140.246 - Mean Length 218.656 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 75.712 - Time 2024-12-12T02:23:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\miniconda3\\envs\\mario\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
      "C:\\Users\\Jorge Casas\\AppData\\Local\\Temp\\ipykernel_7392\\232289090.py:232: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80 - Step 17142 - Epsilon 0.1 - Mean Reward 1101.889 - Mean Length 211.63 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 76.388 - Time 2024-12-12T02:24:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\miniconda3\\envs\\mario\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
      "C:\\Users\\Jorge Casas\\AppData\\Local\\Temp\\ipykernel_7392\\232289090.py:232: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Número de episodios de entrenamiento\n",
    "episodes = 100\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for e in range(episodes):\n",
    "\n",
    "    # Reiniciar el entorno y obtener el estado inicial\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Renderizar el entorno\n",
    "        env.render()\n",
    "\n",
    "        # Elegir una acción con el agente\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Ejecutar la acción en el entorno\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Almacenar la transición en la memoria del agente\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Registrar métricas del paso\n",
    "        logger.log_step(reward, None, None)\n",
    "\n",
    "        # Actualizar el estado actual\n",
    "        state = next_state\n",
    "\n",
    "        # Salir del bucle si el juego termina o Mario alcanza la meta\n",
    "        if done or info['flag_get']:\n",
    "            break\n",
    "\n",
    "    # Registrar métricas del episodio\n",
    "    logger.log_episode()\n",
    "\n",
    "    # Guardar métricas cada 20 episodios\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=mario.exploration_rate,\n",
    "            step=mario.curr_step\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mario)",
   "language": "python",
   "name": "mario"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
